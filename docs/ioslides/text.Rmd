---
title: "GloVe: Global Vectors for Word Representation"
subtitle: "Text Mining Seminar"
author: "Daniel Schalk<span style='padding-left:620px;'>15th March 2017</span>"
output: 
  ioslides_presentation:
    widescreen: true
    css: ioslide.css
    self_contained: no
    includes:
      in_header: header.html
---

```{r, include=FALSE}

knitr::opts_chunk$set(collapse = TRUE)

library(plotly)
library(text2vec)
```


# Outline

## Outline

# Why Word Vectors?

## Mathematical Representation of Words

- Map of each word $i$ to a corresponding word vector $w_i \in \mathbb{R}^d$ 
  (word embedding).
- The word vector $w_i$ can be used to do further analyses e. g. clustering.

<!-- We are not able to do e.g. clustering without having any numeric     -->
<!-- vector which represents the target                                   -->

- Of course we wan't to have useful word vectors $w_i$ (whatever that means).

$\rightarrow$ GloVe is a model which constructs word vectors out of a given 
corpus .

## Goal of GloVe

Model word semantic and analogies between words. Therefore, it is very important
to constitute a linear structure of the word vector space.
<!-- - Difference of vectors are very important here. -->

<center>
```{r, child="rmd_externals/plotly_word_vectors.Rmd"}
```
</center>

# GloVe Model

## Theory: word-word co-occurence Matrix

Base of the model is the word-word co-occurenece matrix 
$X \in \mathbb{R}^{|V| \times |V|}$, where $|V|$ is the number of different 
words which occurs within a given corpus. In $X$ every entry
$X_{ij}$ describes how often word $j$ occurs in context of word $i$ with a given
window size. Therefore we have the following properties:

- $X_i = \sum_k X_{ik}$: Number of words which occur in the context of $i$.

- $P_{ij} = P(j | i) = X_{ij} / X_i$: "Probability" of word $j$ occurs in 
  context of $i$.
  
- The ratio $P_{ik} / P_{jk}$ describes if word $k$ is more related to ...
    - ... word $i$ if  $P_{ik} / P_{jk} > 1$
    - ... word $j$ if $P_{ik} / P_{jk} < 1$
    - ... or similary related if $P_{ik} / P_{jk} \approx 1$
    
<!-- Idea: Words which are related to each other occur more often together -->
<!--       Cat is an animal, dog is an animal, bear is an animal, ...      -->

## Example: word-word co-occurenece Matrix 

**Corpus:** 

<center style="padding-top:5px;padding-bottom:10px;">
A D C E A D F E B A C E D $\ \Rightarrow\ |V| = 5$
</center>

**Window size:** 

<center style="padding-top:5px;padding-bottom:10px;">
2 (the 2 words of the either side) 
</center>

**word-word co-occurenece matrix:**

\[
\begin{array}{c|ccccc}
         & \text{A} & \text{B} & \text{C} & \text{D} & \text{E} \\
\hline
\text{A} &     0    &     1    &     3    &     2    &     3  \\
\text{B} &     1    &     0    &     1    &     0    &     1  \\
\text{C} &     3    &     1    &     0    &     2    &     2  \\
\text{D} &     2    &     0    &     2    &     0    &     4  \\
\text{E} &     3    &     1    &     2    &     4    &     0
\end{array} \ \ \Rightarrow\ \ P_\text{AD} = X_\text{AD} / X_\text{A} = \frac{2}{9}
\]

<!-- In $X$ we now have some word vectors as columns. Why shouldn't we use them
as word vectors?-->

## Where to Start?

Remember: We want to create word vectors $w_i \in \mathbb{R}^d$ and we want to
use the ratio $P_{ik} / P_{jk}$ since the ratio is more appropriate to take 
words into context.

**The Idea:**

\[
F(w_i, w_j, \tilde{w}_k) = \frac{P_{ik}}{P_{jk}}
\]

with word vectors $w_i, w_j$ and context vector $\tilde{w}_k$. $F$ is unknown
at this stage and basically can be any function.

We now parameterize every word vectors, therefore we have $d \cdot |V|$ 
parameter to estimate to get word vectors.

But how should we estimate those parameters without a specific function $F$?

## How does $F$ looks like?

1. Reduce number of possible outputs of $F$ by taking $w_i - w_j$ instead of 
   two vectors $w_i$ and $w_j$:
   \[
   F(w_i - w_j, \tilde{w}_k) = \frac{P_{ik}}{P_{jk}}
   \]

2. We want to keep the linear structure between the word vectors and context
   vector:
   \[
   F\left((w_i - w_j)^T\tilde{w}_k\right) = \frac{P_{ik}}{P_{jk}}
   \]
   
3. Keep exchange symmetrie. We want be able to exchange word vectors $w$ with
   context vectors $\tilde{w}$ without getting different results: \
   $\rightarrow$ not given at this stage
   
<!-- Analogie zum lm oder glm mit linearem Prädiktor. Man modelliert eine -->
<!-- Linearkombination, wodurch in gewissermaßen die interne Stuktur -->
<!-- vorgegeben ist -->
   
## How does $F$ looks like?

To find $F$ and obtain the exchange symmetrie we need to apply two "tricks":

1. We restrict $F$ to be a homomophism between $(\mathbb{R}, f) = (\mathbb{R}, +)$ 
   and $(\mathbb{R}_+, g) = (\mathbb{R}_+, \cdot )$. That means we expect $F$
   to fulfill:
   \[
   F(a + b) = \underbrace{F(f(a, b)) = g(F(a), F(b))}_{\text{Definition of homomorphism}} = F(a)F(b)
   \]
   With $a = w_i^T\tilde{w}_k$ and $b = -w_j^T\tilde{w}_k$. \
   We notice, that the upper equation implies a functional equation which have
   just one solution: 
   \[
   F = \exp_a
   \]
   
## How does $F$ looks like?

To find $F$ and obtain the exchange symmetrie we need to apply two "tricks":

1. We choose $a = \exp(1)$, therefore $F = \exp$. 
   \[
   F\left((w_i - w_j)^T\tilde{w}_k\right) = \exp\left((w_i - w_j)^T\tilde{w}_k\right) = \frac{\exp(w_i^T\tilde{w}_k)}{\exp(w_j^T\tilde{w}_k)} = \frac{P_{ik}}{P_{jk}} \\
   \Rightarrow\ \exp(w_i^T\tilde{w}_k) = P_{ik} = \frac{X_{ik}}{X_i} \\
   \Leftrightarrow\ w_i^T\tilde{w}_k = \log(X_{ik}) - \log(X_i) \\
   \Leftrightarrow\ w_i^T\tilde{w}_k + \log(X_i) = \log(X_{ik})
   \]
   How we handle the case where $X_{ik} = 0$ and therefore no $\log$ is defined
   comes later.
   
## How does $F$ looks like?

To find $F$ and obtain the exchange symmetrie we need to apply two "tricks":

<ol start="2">
<li>
   Since $\log(X_i)$ is independent of $k$ we can put this in a bias term. This
   bias term can decomposed into a term $b_i$ from $w_i$ and one term 
   $\tilde{b}_k$ from $\tilde{w}_k$:
   \[
   \Rightarrow\ w_i^T\tilde{w}_k + b_i + \tilde{b}_k = \log(X_{ik})
   \]
   This is now an model equation which we can use for training the word vectors.
   We also note, that we have transformed the unsupervised task into a 
   supervised task which we know how to handle.
</li>
</ol>

## Model Equation and Loss Function

A problem appears for $X_{ik} = 0$. This is definetily the case since $X$ is 
a sparce matrix. We don't want to drop the sparsity due to very convenient
memory handling. Therefore, we use an additive shift in the logarithm:
\[
\log(X_{ik})\ \rightarrow\ \log(X_{ik} + 1)
\]
This maintains the sparsity: 
\[
X_{ik} = 0\ \Rightarrow\ \log(X_{ik} + 1) = \log(1) = 0
\]
The final objective which we use for training is
\[
w_i^T\tilde{w}_k + b_i + \tilde{b}_k = \log(X_{ik} + 1)
\]

## Model Equation and Loss Function

To estimate the word vectors GloVe uses a weighted least squares appraoch:
\[
J = \sum\limits_{i=1}^{|V|}\sum\limits_{j=1}^{|V|}f(X_{ij})\left(x_i^T\tilde{w}_j + b_i + \tilde{b}_j - \log(X_{ij} + 1)\right)^2
\]
Pennington et. al. proposed as weight function:
\[
f(x) = \left\{
\begin{array}{ccc}
(x / x_\mathrm{max})^\alpha, & x < x_\mathrm{max} \\
1, & x \geq x_\mathrm{max}
\end{array}
\right.
\]

Using this function also introduces two new hyperparameters $\alpha$ and 
$x_\mathrm{max}$. The normal way to find good values for $\alpha$ and 
$x_\mathrm{max}$ would be to use a tuning method . The problem now is that 
for all tuning methods we need to evaluate the model. Since we want to handle an
unsupervised task evaluating isn't straightforward.

<!-- Of course we can try to find lower $J$'s by tuning alpha and x_max.   -->
<!-- But fitting the log counts very good does not necessarily yields good -->
<!-- word vectors which fulfill the desired properties                     -->

<!--
<center>
```{r, child="rmd_externals/plotly_weight_function.Rmd"}
```
</center>
-->

## Algorithm used for Fitting

Basically, GloVe uses a gradient descend technique to minimize the objective 
$J$. Nevertheless, using ordinary gradient descend would be way too expensive
in practise. Therefore some adoptions:

- Individual learning rate in each iteration.

- Adaption of the learning rate to the parameters, performing larger updates 
  for infrequent and smaller updates for frequent parameter

- Well-suited for sparse data, improve robustness of (stochastic) gradient 
  descend
  
- For GloVe, infrequent words require much larger updates than frequent ones

$\rightarrow$ This adaptions leads to a new algorithm called **AdaGrad**

<!--
We perform GloVe fitting using AdaGrad - stochastic gradient descend with 
per-feature adaptive learning rate. Also, fitting is done in fully parallel 
and asynchronous manner ( see Hogwild! paper ), so it can benefit from machines 
with multiple cores. In my tests I achieved almost 8x speedup on 8 core machine 
on the discribed above wikipedia dataset.
-->

# Evaluating GloVe

## Evaluation: Metrics

We need something to measure the distance between vectors:

- **Cosine Similarity**
  \[
  d_\mathrm{cosine}(x,y) = \mathrm{cos}(\angle (x, y)) = \frac{\langle x,y\rangle}{\|x\|_2\|y\|_2}
  \]

- **Euclidean Distance**
  \[
  d_\mathrm{euklid}(x,y) = \sqrt{\sum\limits_{i=1}^n(x_i - y_i)^2}
  \]

In general we could use every metrik (norm) to evaluate the model. But the 
cosine similarity is more robust in terms of curse of dimensionality.

## Evaluation: p-Norm in high Dimensions

<!--
Problem of high dimension:
https://pdfs.semanticscholar.org/f9af/c4590ac7288e722bc154cbfd73be1f575b58.pdf

Explain whats the main result and show simulation and explain the consequences!
-->
CC. Aggarawal et. al. have shown, that in high dimensions the ratio of the 
maximal norm divided by the minimal norm of $n$ points $x_1, \dots, x_n$ 
converges in probability to 1 for increasing dimension $d$:
\[
\underset{{d\rightarrow\infty}}{\mathrm{p~lim}}\ \frac{\mathrm{max}_k \|x_k\|_2}{\mathrm{min}_k \|x_k\|_2} = 1
\]

$\Rightarrow$ Points are centered on the surface of a hypersphere.

## Evaluation: p-Norm in high Dimensions

This can be simulated by setting different dimensions e.g. 3, 5, 10, 100, 500, 
1000, 5000. For each dimension do:

1. Draw $n$ points from a distribution which samples of a distribution (e.g. 
   from hypercube) which returns an $d$ dimensional vector.
   
2. Calculate the minimal and maximal euclidean distance of the $n$ points.

3. Calculate the ratio of the maximal and minimal points.

This can be repeated some times.

## Evaluation: p-Norm in high Dimensions

<center>
```{r, child="rmd_externals/plotly_curse_high_dimension.Rmd"}
```
</center>

## Evaluation: Semantic and Analogies

One thing we now can do is to ask for semantic and analogies between words. 
Something like:
\[
\mathrm{paris\ behaves\ to\ france\ like\ berlin\ to\ ?} \\
\mathrm{animal\ behaves\ to\ animals\ like\ people\ to\ ?} \\
\mathrm{i\ behaves\ to\ j\ like\ k\ to\ l} 
\]
Therefore, we have $3$ given word vectors $w_i$, $w_j$ and $w_k$. To get the 
desired fourth word $l$ we use the linearity of the word vector space:
\[
w_l \approx w_j - w_i + w_k
\]
Furthermore, we obtain $\widehat{l}$ from our model and a given metric 
$d(w_i, w_j)$ (mostly $d = d_\mathrm{cosine}$) by computing:
\[
\widehat{l} = \underset{l \in V}{\mathrm{arg~min}}\ d(w_j - w_i + w_k, w_l)
\]

## Evaluation: Questions Words File

It is obvious that is is not effecient to try this for every possible 
combination and label the result with true or falso to get the overall 
precision. Luckily, some poor guys does exactly that and creates a question
words file which contains about 19000 semantic analogies:

<!--
https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt
-->

<center>
<iframe class="txt" src="../../additional_stuff/questions_words.txt"></iframe>
</center>

## Evaluation: Question Words File

```{r, echo=FALSE, results='asis'}
val.file = readLines("../../additional_stuff/questions_words.txt")

categories.list = list()
categories = val.file[grepl(":", val.file)]

for (i in seq_along(categories)) {
  if (i < length(categories)) {
    categories.list[[substr(categories[i], 3, nchar(categories[i]))]] = val.file[(which(categories[i] == val.file) + 1):(which(categories[i + 1] == val.file) - 1)]  
  } else {
    categories.list[[substr(categories[i], 3, nchar(categories[i]))]] = val.file[(which(categories[i] == val.file) + 1):length(val.file)]
  }
}

examples = data.frame(
  "Category" = names(categories.list),
  "Number of Test Lines" = unlist(lapply(categories.list, length)),
  "Example" = unlist(lapply(categories.list, function (x) { return(x[1])}))
)
colnames(examples) = c("Category", "Number of Test Lines", "Example")

cat("<center>")
knitr::kable(examples, format = "pandoc", row.names = FALSE)
cat("</center>")
```

## Evaluation: Hyperparamter Tuning

- Now tuning is "possible" for a given task specified in the questions words 
  file

- Penningten et al have tuned the model and came to good values (just empirical
  without a proove):
    - $\alpha = 0.75$
    - $x_\mathrm{max} = 100$ (does just have a weakly influence on performance)
    
- Note that we are dependent on this file and can just test on this file. If 
  we want to test other properties of the model we need other files.

# `R` Package `text2vec`

## text2vec: About

- Fast and modern `R` implementation using
    - `R6`: Package for object oriented programming
    - `Rcpp`: `C++` connection for high performance programming, in this case
      counts this is very important in terms of memory.
    - `RcppParallel`: `C++` feature to do parallelization.
    
- In some functions maintenance is not very good. Some functions need
  further preparations or adaptions

## Important Commands: Preprocessing

- Create an iterator which separates by whitespaces:
    ```{r}
    space_tokenizer("Hello my name is")
    ```

- Create and prune vocabulary:
    ```{r, eval=FALSE}
    iterator   = itoken(tokens, progressbar = TRUE)
    vocabulary = create_vocabulary(iterator)
    vocabulary = prune_vocabulary(vocabulary, term_count_min = 10L)
    ```

- Map the words to indizes and create token count matrix:
    ```{r, eval=FALSE}
    vectorizer = vocab_vectorizer(vocab)
    tcm = create_tcm(iterator, vectorizer, skip_grams_window = 20L)
    ```

## Important Commands: Train the Model

`GloVe` or `GlobalVectors` are `R6` classes. First, it is necessary to create
an new object of the class:

```{r, eval=FALSE}
# All specifica are setted here:
glove = GlobalVectors$new(
  word_vectors_size = 300,        # Dimension d of word vectors
  vocabulary        = vocabulary, # Used vocabulary
  learning_rate     = 0.03,       # Learning rate for Adagrad
  alpha             = 3/4,
  x_max             = 100
)
```

The training and extraction of word vectors then is done by:

```{r, eval=FALSE}
glove$fit_transform(tcm, n_iter = 50) # Adagrad with 50 iterations
word.vectors = glove$components       # Extract word vectors
```

# Data and Usecase

## Data: The Language

- Be careful with the language: 

<center>

| German term list              | English term list         |
| ----------------------------- | ------------------------- |
| Wassermolekuel                | hydrogen                  | 
| Wasserstoff                   | hydrogen-bonding          |
| Wasserstoffatom               |                           |
| Wasserstoffbindung            |                           |
| Wasserstoffbrueckenbildung    |                           | 
| Wasserstoffbrueckenbindung    |                           |
| Wasserstoffhalogenid          |                           |
| Wasserstoffverbindung         |                           |

<p class="caption">
Copied from Grammar & Corpora 2009. Excerpt of the resulting English and 
German term lists focusing on the term *hydrogen* (German: *Wasserstoff*).
</p>

<!-- https://books.google.de/books?id=PqHOU-cF674C&printsec=frontcover&hl=de#v=onepage&q&f=false -->

</center>
    
&nbsp;&nbsp;&nbsp;&nbsp;For instance, German has much more rare words and 
a bigger variety (harder &nbsp;&nbsp;&nbsp;&nbsp;for modelling) than English.

## Data: The Corpus

- We need a lot of words to train the model.

- Often crawled from the web. This is mostly followed by a lot of 
  preprocessing (regular expressions, filter stopwords etc.).
  
<!-- - Stopwords from http://snowballstem.org -->

But, how big should the corbus and the vocabular be to get good word vectors?

## Data: Corpus Size
  
Common sources for the corpus are:
  
- **Wikipedia Dump:** Wikpedia gives access to all articles collected within
  one XML file (unzipped about 63 GB). <br><br>
  $\rightarrow$ 6 billion tokens and 400 thousand word vocabulary
  
<!-- reduced simple wikipedia dump -->

- **Common Crawl:** Published by Amazon Web Services through its Public Data 
  Sets program in 2012. The data was crawled from the whole web and contains 
  2.9 billion web pages and over 240 TB of data. <br><br>
  $\rightarrow$ 820 billion tokens and 2.2 million word vocabulary
  
<!-- http://commoncrawl.org/connect/blog/ -->

<!-- Say something about the resulting term count matrix --> 
<!-- (size of vocabulary to the power of two).           --> 
<!-- Expected size:                                      --> 
<!--     - Wikipedia:    ~3.3 GB                         --> 
<!--     - Common Crawl:  ~18 GB                         -->




## Usecase

- Ähnlichkeiten: Leicht anfangen, d. h. einfach in euklidischer Norm 
  illustrieren mit Bildern
  
- Ähnlichkeiten: women to queen wie men to ???

- `d3` Visualisierungen raus ballern (clustern, ...)

# Outlook

## Outlook

- Clustern von Artikeln nach Wörtern


# Ideas

## Todos Visualization

- Nachdem man den Text ausgewertet hat sollen die W?rter analog zu
  [diesem Beispiel](https://bl.ocks.org/mbostock/1044242) visualisiert werden.
  Dabei sollen ?nnliche W?rter zusammen geclustert sein nach GloVe oder SVD oder
  sonst was. Geht man dann auf ein Wort sollen die "most frequent words" in dem
  Kontext zu dem Wort auf dem die Maus ist angezeigt werden.

# <i class="fa fa-sitemap"></i> Class System

## <i class="fa fa-sitemap"></i> 2 Approaches

<div class="cols">
  <div class="box1">
  <center>
  <font size="7">C++</font>
  </center>
  </div>
  <div class="box2">
  <p>Solid <code>C++</code> base with </p>
  <ul>
  <li> Own class system for "frequently used" base learner </li>
  <li> Give the opportunity to extend the the functionality with own 
       <code>C++</code> functions </li>
  </ul>
  </div>
</div>

<br>

<div class="cols">
  <div class="box1">
  <center>
  <font size="7">R</font>
  </center>
  </div>
  <div class="box2">
  <p><code>R</code> API and wrapper around the core <code>C++</code> functionality:</p>
  <ul>
  <li> Basically the same as in <code>C++</code> but much more easier to extend 
       with own base learner and loss/gradient </li>
  <li> Functions to summarize, visualize the results and to refit the
       model etc.</li>
  </ul>
  </div>
</div>
