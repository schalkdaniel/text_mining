\chapter{About the Data}\label{ch:data}

\section{The Language}

Be careful with the language. For instance, German has much more rare words and a
bigger variety (harder for modelling) than English since more words become more 
rare. Table \ref{tab:ger-vs-eng-terms} shows an example of comparing related words 
in both languages focusing on the term hydrogen.

\begin{table}[!h]
\begin{center}
\begin{tabular}{ l | l }
\hline
\textbf{German term list} & \textbf{English term list} \\
\hline\hline
Wassermolekuel                & hydrogen                  \\ \hline
Wasserstoff                   & hydrogen-bonding          \\ \hline
Wasserstoffatom               &                           \\ \hline
Wasserstoffbindung            &                           \\ \hline
Wasserstoffbrueckenbildung    &                           \\ \hline
Wasserstoffbrueckenbindung    &                           \\ \hline
Wasserstoffhalogenid          &                           \\ \hline
Wasserstoffverbindung         &                           \\ \hline
\end{tabular}
\caption[Term list comparison between German and English]{
         Copied from Grammar \& Corpora 2009 \cite{konopka2011grammar}. Excerpt of 
         the resulting English and German term lists focusing on the term 
         \textit{hydrogen} (German: \textit{Wasserstoff}).}
\label{tab:ger-vs-eng-terms}
\end{center}
\end{table}



\section{Common Sources}\label{sec:common-sources}

To train word embeddings we need a lot of words to train those models. GloVe makes
no difference. Common sources have multiple billion words an a total size of 
multiple gigabytes. To get that huge amount of words the common way is to crawl
from the internet. This is mostly followed by a lot of preprocessing such as 
removing regular expressions (HTML syntax) or filtering stop words (very common
words as the or this). \\

This also imply two question which has been asked:
\begin{enumerate}
  \item 
    How big should the corpus and the vocabular be to get good word vectors?
  
  \item 
    Does different corpora imply a different quality of the word vectors?
\end{enumerate}

We want to answer the first one now while the second one will be answered in 
section \ref{ch:real_wv}. The most common sources for word embeddings are the
following (tokens all words within the corpus while the vocabulary 
describes the number of unique words $|V|$):

\begin{itemize}
  \item 
    \textbf{Wikipedia Dump + Gigaword 5:} Wikipedia gives access to all articles 
    collected within one XML file (unzipped about 63 GB). \cite{pennington2014glove} 
    combines this with Gigaword 5, an archive of newswire text data. \\ \\
    $\rightarrow$ 6 billion tokens and 400 thousand word vocabulary

  \item 
    \textbf{Common Crawl:} Published by Amazon Web Services through its Public Data 
    Sets program in 2012. The data was crawled from the whole web and contains 
    2.9 billion web pages and over 240 TB of data. \\ \\
    $\rightarrow$ 42 billion tokens and 1.9 million word vocabulary or \\
    $\rightarrow$ 820 billion tokens and 2.2 million word vocabulary
  
  \item 
    \textbf{Twitter:} \cite{pennington2014glove} crawled 2 billion tweets. \\ \\
    $\rightarrow$ 27 billion tokens and 1.2 million word vocabulary
\end{itemize}
  
