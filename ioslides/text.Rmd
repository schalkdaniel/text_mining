---
title: "GloVe: Global Vectors for Word Representation"
subtitle: "Text Mining Seminar"
author: "Daniel Schalk<span style='padding-left:620px;'>15th. March 2017</span>"
output: 
  ioslides_presentation:
    widescreen: true
    css: ioslide.css
    self_contained: no
    includes:
      in_header: header.html
---

```{r, include=FALSE}
library(plotly)
```


# Outline

## Outline

# Why Word Vectors?

## Mathematical Representation of Words

- Map of each word $i$ to a corresponding word vector $w_i \in \mathbb{R}^d$.
- The word vector $w_i$ can be used to do further analyses e. g. clustering.
- Of course we wan't to have useful word vectors $w_i$ (whatever that means).

$\rightarrow$ GloVe is a model which constructs word vectors out of a given 
corpus.

## Goal of GloVe

Model word semantic and analogies between words. Therefore, it is very important
to constitute a linear structure of the word vector space.
<!-- - Difference of vectors are very important here. -->

<center>
```{r, child="rmd_externals/plotly_word_vectors.Rmd"}
```
</center>

# GloVe Model

## Theory

- Where to start
- Where to go
- Assumptions, restrictions etc. 
- Which algorithm is used to fit the model?

We perform GloVe fitting using AdaGrad - stochastic gradient descend with 
per-feature adaptive learning rate. Also, fitting is done in fully parallel 
and asynchronous manner ( see Hogwild! paper ), so it can benefit from machines 
with multiple cores. In my tests I achieved almost 8x speedup on 8 core machine 
on the discribed above wikipedia dataset.

- Which hyperparameter are recommended (see paper)

## Evaluation

- Explain "Cosine Similarity" (Example with two vectors in 2d)
- Further evaluation with questions-words file (men to king like women to ?)

# `R` Implementation `text2vec`

## `text2vec`

- implementation `R6`, `Rcpp` (`C++`), `RcppParallel` (modern + clean)
- Explain important commands
- Lack of maintainance

# Data and Usecase

## Data

- Be careful with the language: 

<center>

| German term list             | English term list         |
| ---------------------------- | ------------------------- |
| Wassermolekül                | hydrogen                  | 
| Wasserstoff                  | hydrogen-bonding          |
| Wasserstoffatom              |                           |
| Wasserstoffbindung           |                           |
| Wasserstoffbrückenbildung    |                           | 
| Wasserstoffbrückenbindung    |                           |
| Wasserstoffhalogenid         |                           |
| Wasserstoffverbindung        |                           |

<p class="caption">
Copied from Grammar & Corpora 2009. Excerpt of the resulting English and 
German term lists focusing on the term *hydrogen* (German: *Wasserstoff*).
</p>

<!-- https://books.google.de/books?id=PqHOU-cF674C&printsec=frontcover&hl=de#v=onepage&q&f=false -->

</center>
    
&nbsp;&nbsp;&nbsp;&nbsp;For instance, German has much more rare words and 
a bigger variety (harder &nbsp;&nbsp;&nbsp;&nbsp;for modelling) than English.

## Data

- We need a lot of words to train the model.
- Often crawled from the web. This is then mostly followed by a lot of 
  preprocessing (regular expressions, filter stopwords etc.).
  
<!-- - Stopwords from http://snowballstem.org -->

But, how big should the corbus and the vocabular be to get good word vectors?

## Data
  
Common sources for the corpus are:
  
- **Wikipedia Dump:** Wikpedia gives accesss to all articles collected within
  one XML file (unzipped about 63 GB). <br><br>
  $\rightarrow$ 6 billion tokens and 400 thousand word vocabulary
  
<!-- reduced simple wikipedia dump -->

- **Common Crawl:** Published by Amazon Web Services through its Public Data 
  Sets program in 2012. The data was crawled from the whole web and contains 
  2.9 billion web pages and over 240 TB of data. <br><br>
  $\rightarrow$ 820 billion tokens and 2.2 million word vocabulary
  
<!-- http://commoncrawl.org/connect/blog/ -->

<!-- Say something about the resulting term count matrix --> 
<!-- (size of vocabulary to the power of two).           --> 
<!-- Expected size:                                      --> 
<!--     - Wikipedia:    ~3.3 GB                         --> 
<!--     - Common Crawl:  ~18 GB                         -->

## Usecase

- Ã„hnlichkeiten: Leicht anfangen, d. h. einfach in euklidischer Norm 
  illustrieren mit Bildern
  
- Ã„hnlichkeiten: women to queen wie men to ???

- `d3` Visualisierungen raus ballern (clustern, ...)

# Outlook

## Outlook

- Clustern von Artikeln nach WÃ¶rtern


# Ideas

## Todos Visualization

- Nachdem man den Text ausgewertet hat sollen die W?rter analog zu
  [diesem Beispiel](https://bl.ocks.org/mbostock/1044242) visualisiert werden.
  Dabei sollen ?nnliche W?rter zusammen geclustert sein nach GloVe oder SVD oder
  sonst was. Geht man dann auf ein Wort sollen die "most frequent words" in dem
  Kontext zu dem Wort auf dem die Maus ist angezeigt werden.

# <i class="fa fa-sitemap"></i> Class System

## <i class="fa fa-sitemap"></i> 2 Approaches

<div class="cols">
  <div class="box1">
  <center>
  <font size="7">C++</font>
  </center>
  </div>
  <div class="box2">
  <p>Solid <code>C++</code> base with </p>
  <ul>
  <li> Own class system for "frequently used" base learner </li>
  <li> Give the opportunity to extend the the functionality with own 
       <code>C++</code> functions </li>
  </ul>
  </div>
</div>

<br>

<div class="cols">
  <div class="box1">
  <center>
  <font size="7">R</font>
  </center>
  </div>
  <div class="box2">
  <p><code>R</code> API and wrapper around the core <code>C++</code> functionality:</p>
  <ul>
  <li> Basically the same as in <code>C++</code> but much more easier to extend 
       with own base learner and loss/gradient </li>
  <li> Functions to summarize, visualize the results and to refit the
       model etc.</li>
  </ul>
  </div>
</div>
